{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Done\n",
    "* Forward pass\n",
    "* Backwards pass\n",
    "* Summaries\n",
    "* Replay buffer\n",
    "* target network\n",
    "* Save model\n",
    "* Breakout \n",
    "* preprocessing\n",
    "\n",
    "\n",
    "### Missing\n",
    "* Action repeats\n",
    "* TensorBoard Summaries\n",
    "* Let it train\n",
    "\n",
    "### Notes\n",
    "Inspired by: https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on Breakout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "Jupyter.keyboard_manager.command_shortcuts.add_shortcut('r', {\n",
    "    help : 'run all cells',\n",
    "    help_index : 'zz',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_all_cells();\n",
    "        return false;\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from tensorflow.contrib.keras.api.keras.layers import Dense, Input, Conv2D, Flatten\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "update_frequency = 4\n",
    "batch_size = 32\n",
    "training_time = int(50e6) # frames\n",
    "# training_time = int(1e6) # frames\n",
    "target_size = [84,84] #List of integer\n",
    "num_frames = 4 # Integer\n",
    "gray = True\n",
    "\n",
    "replay_buffer_size = int(1e5)\n",
    "# replay_buffer_size = int(1e5)\n",
    "minimum_experience = replay_buffer_size/10\n",
    "\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99\n",
    "epsilon_init = 1.0\n",
    "epsilon_end = 0.1\n",
    "\n",
    "annealing_period = 1e6 # frames\n",
    "# annealing_period = 5e5 # frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parameters\n",
    "if 1:\n",
    "    env_name = 'Breakout-v0'\n",
    "    model_type = 'DQN'\n",
    "elif 0:\n",
    "    env_name = 'CartPole-v1'\n",
    "    model_type = 'dense'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Derived settings\n",
    "tf.reset_default_graph()\n",
    "annealer = utils.Annealer(epsilon_init, epsilon_end, annealing_period)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "s_size = list(obs.shape)\n",
    "a_size = env.action_space.n\n",
    "n_channels = 1 if gray else 4\n",
    "target_dim = target_size + [num_frames*n_channels]\n",
    "print('-- Environmental variables --')\n",
    "print('env_name  ', env_name)\n",
    "print('model_type', model_type)\n",
    "print('s_size    ', s_size)\n",
    "print('a_size    ', a_size)\n",
    "print('target_dim', target_dim)\n",
    "\n",
    "obsPlaceholder = tf.placeholder(tf.float32, shape=[None]+target_dim, name='obsPlaceholder')\n",
    "# Assume action is encoded as ONE number\n",
    "actionPlaceholder = tf.placeholder(tf.int32, shape=[None], name='actionPlaceholder')\n",
    "targetQPlaceholder = tf.placeholder(tf.float32, shape=[None], name='targetQPlaceholder')\n",
    "\n",
    "print('\\n-- Placeholders --')\n",
    "print('obsPlaceholder    ', obsPlaceholder.get_shape())\n",
    "print('actionPlaceholder ', actionPlaceholder.get_shape())\n",
    "print('targetQPlaceholder', targetQPlaceholder.get_shape())\n",
    "print()\n",
    "\n",
    "### Create model\n",
    "from models import DQN\n",
    "with tf.name_scope('DQN1'):\n",
    "    DQN1 = DQN(model_type, obsPlaceholder, actionPlaceholder, a_size)\n",
    "    DQN1.create_MSE_train_op(targetQPlaceholder, learning_rate=learning_rate)\n",
    "\n",
    "with tf.name_scope('DQN2'):\n",
    "    DQN2 = DQN(model_type, obsPlaceholder, actionPlaceholder, a_size)\n",
    "    DQN2.create_MSE_train_op(targetQPlaceholder, learning_rate=learning_rate)\n",
    "\n",
    "print('Model summary')\n",
    "DQN1.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessor\n",
    "preprocessor = utils.DataHandler()\n",
    "preprocessor.define_preprocess_2d(target_size=target_size, num_frames=num_frames, gray=gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Stuff for saving the model\n",
    "path = \"./dqn\" #The path to save our model to.\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Script specific helper functions\n",
    "def new_episode(env):\n",
    "    \"\"\"Simple wrapper that restarts the environment\"\"\"\n",
    "    obs = env.reset()\n",
    "    episode_time_step = 0\n",
    "    episode_reward = 0\n",
    "    return obs, episode_time_step, episode_reward\n",
    "\n",
    "\n",
    "## Stuff for updating the target graph\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    # TODO: FIX THIS\n",
    "    # This is a terrible way of doing it, it relies on the order in which the networks were created\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = utils.Experience_buffer(replay_buffer_size)\n",
    "f = 0\n",
    "training_summaries = {}\n",
    "training_summaries['num_ep'] = 0\n",
    "training_summaries['ep_rewards'] = []\n",
    "training_summaries['epsilon'] = []\n",
    "\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "assert update_frequency > 1, \"The dimensions doen't allign if update_frequency\" \\\n",
    "    + 'is less than 1: ' + str(update_frequency)\n",
    "\n",
    "\n",
    "try:\n",
    "    print('Session restarted')\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# TODO: Use TF summaries, this is shit\n",
    "\n",
    "if load_model == True:\n",
    "    print('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    try:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    except AttributeError:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        print('WARNING: Could not load previous model')\n",
    "\n",
    "else:\n",
    "    if os.path.exists(path):\n",
    "        print('Deleting old model')\n",
    "        shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "\n",
    "preprocessor.reset_buffer_2d()\n",
    "obs, ep_t, ep_r = new_episode(env)\n",
    "preprocessor.add_2d(obs)\n",
    "obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "while f < training_time:\n",
    "    try:\n",
    "        f += 1\n",
    "        epsilon = annealer.linear(f-minimum_experience)\n",
    "        action = sess.run(DQN1.action, feed_dict={obsPlaceholder : [obs_input]}) \\\n",
    "            if np.random.rand(1) > epsilon else np.random.randint(a_size)\n",
    "        action = int(action)\n",
    "#         print('action', type(action), action)\n",
    "        assert 'int' in str(type(action)), 'action must be an int, not a ' + str(type(action))\n",
    "        reward = 0\n",
    "        for i in range(4):\n",
    "            next_obs, r, done, _ = env.step(action)\n",
    "            r = np.clip(r, -1, 1)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "        preprocessor.add_2d(next_obs)\n",
    "        next_obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "        # TODO: wrap in action repeater\n",
    "        ep_t += 1\n",
    "        ep_r += reward\n",
    "        if 'CartPole' in env_name and done and ep_t < 400 :\n",
    "            # Necessary help for he Q-network to avoid death\n",
    "            reward = -1\n",
    "\n",
    "        experience = {'obs':[obs_input], 'action':action, 'reward':reward,\n",
    "                      'next_obs':[next_obs_input], 'done':done}\n",
    "        buffer.add(experience)\n",
    "        obs = next_obs\n",
    "        \n",
    "        ## Update Weights\n",
    "        if f % update_frequency == 0 \\\n",
    "                and f > minimum_experience:\n",
    "            train_batch = buffer.sample(batch_size)\n",
    "\n",
    "            # Compute ... TODO: add description\n",
    "            [action] = sess.run([DQN1.action],\n",
    "                feed_dict={obsPlaceholder : train_batch['next_obs']})\n",
    "            [actualQs] = sess.run([DQN2.Qout],\n",
    "                feed_dict={obsPlaceholder : train_batch['next_obs']})\n",
    "\n",
    "            actualQ = actualQs[range(batch_size), action] # The Q value of the chosen action\n",
    "            zero_if_done = train_batch['done']*(-1) + 1 # Used to remove the actualQ\n",
    "                                                        # When at terminal state\n",
    "            target = train_batch['reward'] + gamma*actualQ*zero_if_done\n",
    "\n",
    "            ## Update DQN1\n",
    "            DQN1_train_dict = {\n",
    "                        obsPlaceholder : train_batch['obs'],\n",
    "                        actionPlaceholder : train_batch['action'],\n",
    "                        targetQPlaceholder : target # TODO: Should be target network!\n",
    "                }\n",
    "            _ = sess.run(DQN1.train_op, feed_dict=DQN1_train_dict)\n",
    "            \n",
    "            ## Update DQN2\n",
    "            updateTarget(targetOps,sess)\n",
    "        \n",
    "        ## Track training\n",
    "        track_interval = 1000\n",
    "        if f % track_interval == 0:\n",
    "            if f==0:\n",
    "                print('\\n{:>9}  {:>7}  {:>7}  {:>7}'.format('frames', 'epis', 'reward', 'epsilon'),end='')\n",
    "            else:\n",
    "                print('{:9}, {:7}, {:7.1f}, {:7.3f}'.format(\n",
    "                      f, \n",
    "                      training_summaries['num_ep'],\n",
    "                      np.mean(training_summaries['ep_rewards'][-100:]),\n",
    "                      training_summaries['epsilon'][-1],\n",
    "                     ), end='')\n",
    "            if f % (track_interval*5) == 0 and f>0:\n",
    "                i = training_summaries['num_ep']\n",
    "                model_save_str = path+'/model-'+str(i)+'.cptk'\n",
    "                saver.save(sess, model_save_str)\n",
    "                print(\"\\tSaved Model: \" + model_save_str)\n",
    "            else:\n",
    "                print()\n",
    "        \n",
    "        if training_summaries['num_ep'] % 100 == 0:\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            training_summaries['num_ep'] += 1\n",
    "            training_summaries['ep_rewards'].append(ep_r)\n",
    "            training_summaries['epsilon'].append(epsilon)\n",
    "            preprocessor.reset_buffer_2d()\n",
    "            obs, ep_t, ep_r = new_episode(env)\n",
    "            preprocessor.add_2d(obs)\n",
    "            obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        \n",
    "\n",
    "env.render(close=True)\n",
    "print('\\nTerminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(buffer.buffer_size())\n",
    "sys.getsizeof(buffer.buffer) # bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvironmentInterface():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def define_take_action(self):\n",
    "        pass\n",
    "    \n",
    "    def take_action(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run and render a forward pass\n",
    "import time\n",
    "\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "preprocessor.reset_buffer_2d()\n",
    "preprocessor.add_2d(obs)\n",
    "obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "env.render()\n",
    "reward_sum = 0\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        action = sess.run(DQN1.action, feed_dict={obsPlaceholder : [obs_input]}) if np.random.rand(1) > epsilon \\\n",
    "            else np.random.randint(a_size)\n",
    "        action = int(action)\n",
    "        assert 'int' in str(type(action))\n",
    "        time.sleep(0.025)\n",
    "        reward = 0\n",
    "        for i in range(4):\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            r = np.clip(r, -1, 1)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "        preprocessor.add_2d(obs)\n",
    "        obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "        env.render()\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(reward_sum)\n",
    "            reward_sum = 0\n",
    "            obs = env.reset()\n",
    "            preprocessor.reset_buffer_2d()\n",
    "            preprocessor.add_2d(obs)\n",
    "            obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "# env.render(close=True)\n",
    "print('Terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
