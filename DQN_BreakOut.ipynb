{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Done\n",
    "* Forward pass\n",
    "* Backwards pass\n",
    "* Summaries\n",
    "* Replay buffer\n",
    "* target network\n",
    "* Save model\n",
    "* Breakout \n",
    "* preprocessing\n",
    "\n",
    "\n",
    "### Missing\n",
    "* Action repeats\n",
    "* TensorBoard Summaries\n",
    "    * Track episode lengths!\n",
    "* Set max episode length! (double of the mean?)\n",
    "* Let it train\n",
    "* ? use int8 to store the experiences? ~ x4 memory saver\n",
    "\n",
    "### Notes\n",
    "Inspired by: https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on Breakout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "Jupyter.keyboard_manager.command_shortcuts.add_shortcut('r', {\n",
    "    help : 'run all cells',\n",
    "    help_index : 'zz',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_all_cells();\n",
    "        return false;\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "from tensorflow.contrib.keras.api.keras.layers import Dense, Input, Conv2D, Flatten\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Hyperparameters\n",
    "update_frequency = 4\n",
    "batch_size = 16\n",
    "training_time = int(50e6) # frames\n",
    "target_size = [84,84] #List of integer\n",
    "num_frames = 4 # Integer\n",
    "gray = True # Make obs grayscale \n",
    "\n",
    "replay_buffer_size = int(1e6) # DOESN'T FIT in memory\n",
    "replay_buffer_size = int(2e5)\n",
    "minimum_experience = replay_buffer_size\n",
    "\n",
    "learning_rate = 0.0001\n",
    "gamma = 0.99\n",
    "epsilon_init = 1.0\n",
    "epsilon_end = 0.1\n",
    "\n",
    "annealing_period = 1e6 # frames\n",
    "# annealing_period = 5e5 # frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Parameters\n",
    "if 1:\n",
    "    env_name = 'Breakout-v0'\n",
    "    model_type = 'DQN'\n",
    "    path = \"./dqn\" #The path to save our model to.\n",
    "    \n",
    "else:\n",
    "    env_name = 'CartPole-v1'\n",
    "    model_type = 'dense'\n",
    "    path = './dense'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Derived settings\n",
    "tf.reset_default_graph()\n",
    "annealer = utils.Annealer(epsilon_init, epsilon_end, annealing_period)\n",
    "\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "s_size = list(obs.shape)\n",
    "a_size = env.action_space.n\n",
    "n_channels = 1 if gray else 4\n",
    "target_dim = target_size + [num_frames*n_channels]\n",
    "print('-- Environmental variables --')\n",
    "print('env_name  ', env_name)\n",
    "print('model_type', model_type)\n",
    "print('s_size    ', s_size)\n",
    "print('a_size    ', a_size)\n",
    "print('target_dim', target_dim)\n",
    "\n",
    "obsPlaceholder = tf.placeholder(tf.float32, shape=[None]+target_dim, name='obsPlaceholder')\n",
    "obsPlaceholder = tf.placeholder(tf.float32, shape=[None]+target_dim, name='obsPlaceholder')\n",
    "# Assume action is encoded as ONE number\n",
    "actionPlaceholder = tf.placeholder(tf.int32, shape=[None], name='actionPlaceholder')\n",
    "targetQPlaceholder = tf.placeholder(tf.float32, shape=[None], name='targetQPlaceholder')\n",
    "\n",
    "print('\\n-- Placeholders --')\n",
    "print('obsPlaceholder    ', obsPlaceholder.get_shape())\n",
    "print('actionPlaceholder ', actionPlaceholder.get_shape())\n",
    "print('targetQPlaceholder', targetQPlaceholder.get_shape())\n",
    "print()\n",
    "\n",
    "### Create model\n",
    "from models import DQN\n",
    "with tf.name_scope('DQN1'):\n",
    "    DQN1 = DQN(model_type, obsPlaceholder, actionPlaceholder, a_size)\n",
    "    DQN1.create_MSE_train_op(targetQPlaceholder, learning_rate=learning_rate)\n",
    "\n",
    "with tf.name_scope('DQN2'):\n",
    "    DQN2 = DQN(model_type, obsPlaceholder, actionPlaceholder, a_size)\n",
    "    DQN2.create_MSE_train_op(targetQPlaceholder, learning_rate=learning_rate)\n",
    "\n",
    "print('Model summary')\n",
    "DQN1.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preprocessor\n",
    "preprocessor = utils.DataHandler()\n",
    "preprocessor.define_preprocess_2d(target_size=target_size, num_frames=num_frames, gray=gray)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Stuff for saving the model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Script specific helper functions\n",
    "\n",
    "## Stuff for updating the target graph\n",
    "def updateTargetGraph(tfVars,tau):\n",
    "    # TODO: FIX THIS\n",
    "    # This is a terrible way of doing it, it relies on the order in which the networks were created\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)\n",
    "\n",
    "tau = 0.001 #Rate to update target network toward primary network\n",
    "trainables = tf.trainable_variables()\n",
    "targetOps = updateTargetGraph(trainables,tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = utils.Experience_buffer(replay_buffer_size)\n",
    "f = 0\n",
    "training_summaries = {}\n",
    "training_summaries['num_ep'] = 0\n",
    "training_summaries['ep_rewards'] = []\n",
    "training_summaries['epsilon'] = []\n",
    "\n",
    "load_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def current_time():\n",
    "    return '{:2}:{:<2}'.format(datetime.now().hour, datetime.now().minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert update_frequency > 1, \"The dimensions doen't allign if update_frequency\" \\\n",
    "    + 'is less than 1: ' + str(update_frequency)\n",
    "\n",
    "\n",
    "try:\n",
    "    print('Session restarted')\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# TODO: Use TF summaries, this is shit\n",
    "\n",
    "if load_model == True:\n",
    "    print('Loading Model...')\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    try:\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    except AttributeError:\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        print('WARNING: Could not load previous model')\n",
    "\n",
    "else:\n",
    "    if os.path.exists(path):\n",
    "        print('Deleting old model')\n",
    "        shutil.rmtree(path)\n",
    "    os.mkdir(path)\n",
    "\n",
    "preprocessor.reset_buffer_2d()\n",
    "obs, ep_t, ep_r = new_episode(env)\n",
    "preprocessor.add_2d(obs)\n",
    "obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        if f == minimum_experience:\n",
    "            print('___/// BEGIN TRAINING \\\\\\___')\n",
    "        epsilon = annealer.linear(f-minimum_experience)\n",
    "        action = sess.run(DQN1.action, feed_dict={obsPlaceholder : [obs_input]}) \\\n",
    "            if np.random.rand(1) > epsilon else np.random.randint(a_size)\n",
    "        action = int(action)\n",
    "#         print('action', type(action), action)\n",
    "        assert 'int' in str(type(action)), 'action must be an int, not a ' + str(type(action))\n",
    "        reward = 0\n",
    "        for i in range(4):\n",
    "            next_obs, r, done, _ = env.step(action)\n",
    "            r = np.clip(r, -1, 1)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "        preprocessor.add_2d(next_obs)\n",
    "        next_obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "        # TODO: wrap in action repeater\n",
    "        ep_t += 1\n",
    "        ep_r += reward\n",
    "        if 'CartPole' in env_name and done and ep_t < 400 :\n",
    "            # Necessary help for he Q-network to avoid death\n",
    "            reward = -1\n",
    "\n",
    "        experience = {'obs':[obs_input], 'action':action, 'reward':reward,\n",
    "                      'next_obs':[next_obs_input], 'done':done}\n",
    "        buffer.add(experience)\n",
    "        obs = next_obs\n",
    "        \n",
    "        ## Update Weights\n",
    "        if f % update_frequency == 0 \\\n",
    "                and f > minimum_experience:\n",
    "            train_batch = buffer.sample(batch_size)\n",
    "\n",
    "            # Compute ... TODO: add description\n",
    "            [action] = sess.run([DQN1.action],\n",
    "                feed_dict={obsPlaceholder : train_batch['next_obs']})\n",
    "            [actualQs] = sess.run([DQN2.Qout],\n",
    "                feed_dict={obsPlaceholder : train_batch['next_obs']})\n",
    "\n",
    "            actualQ = actualQs[range(batch_size), action] # The Q value of the chosen action\n",
    "            zero_if_done = train_batch['done']*(-1) + 1 # Used to remove the actualQ\n",
    "                                                        # When at terminal state\n",
    "            target = train_batch['reward'] + gamma*actualQ*zero_if_done\n",
    "\n",
    "            ## Update DQN1\n",
    "            DQN1_train_dict = {\n",
    "                        obsPlaceholder : train_batch['obs'],\n",
    "                        actionPlaceholder : train_batch['action'],\n",
    "                        targetQPlaceholder : target # TODO: Should be target network!\n",
    "                }\n",
    "            _ = sess.run(DQN1.train_op, feed_dict=DQN1_train_dict)\n",
    "            \n",
    "            ## Update DQN2\n",
    "            updateTarget(targetOps,sess)\n",
    "        \n",
    "        ## Track training\n",
    "        track_interval = 5000\n",
    "        if f % track_interval == 0:\n",
    "            if f % (track_interval*10) == 0:\n",
    "                print('\\n{:5} {:>9}  {:>7}  {:>7}  {:>7}'.format(\n",
    "                    'time', 'frames', 'epis', 'reward', 'epsilon'))\n",
    "            else:\n",
    "                print('{:5} {:9}, {:7}, {:7.1f}, {:7.3f}'.format(\n",
    "                      current_time(),\n",
    "                      f, \n",
    "                      training_summaries['num_ep'],\n",
    "                      np.mean(training_summaries['ep_rewards'][-100:]),\n",
    "                      training_summaries['epsilon'][-1],\n",
    "                     ), end='')\n",
    "            if f % (track_interval*10) == 0 and f>0:\n",
    "                i = training_summaries['num_ep']\n",
    "                model_save_str = path+'/model-'+str(i)+'.cptk'\n",
    "                saver.save(sess, model_save_str)\n",
    "                print(\"Saved Model: \" + model_save_str)\n",
    "            else:\n",
    "                print()\n",
    "        \n",
    "        if training_summaries['num_ep'] % 100 == 0:\n",
    "            env.render()\n",
    "\n",
    "        if done:\n",
    "            training_summaries['num_ep'] += 1\n",
    "            training_summaries['ep_rewards'].append(ep_r)\n",
    "            training_summaries['epsilon'].append(epsilon)\n",
    "            preprocessor.reset_buffer_2d()\n",
    "            obs, ep_t, ep_r = new_episode(env)\n",
    "            preprocessor.add_2d(obs)\n",
    "            obs_input = preprocessor.get_buffer_2d()\n",
    "        f += 1\n",
    "        \n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        \n",
    "\n",
    "env.render(close=True)\n",
    "print('\\nTerminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some memory evaluation, that doesn't seem to work...\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "experience_size = 0\n",
    "experience_size += sys.getsizeof(buffer.buffer[0]['action'])\n",
    "experience_size += sys.getsizeof(buffer.buffer[0]['done'])\n",
    "experience_size += sys.getsizeof(buffer.buffer[0]['obs'][0])\n",
    "experience_size += sys.getsizeof(buffer.buffer[0]['next_obs'][0])\n",
    "\n",
    "total_ram = 30 * 1024 * 1024 * 1024\n",
    "buffer_len = buffer.buffer_size()\n",
    "\n",
    "print('buffer len', buffer_len)\n",
    "print('experience_size', sizeof_fmt(experience_size))\n",
    "print('buffer memory', sizeof_fmt(experience_size*buffer_len))\n",
    "print('total ram', sizeof_fmt(total_ram))\n",
    "print('buffer max', sizeof_fmt(experience_size * replay_buffer_size))\n",
    "print('Deepmind memory', sizeof_fmt(experience_size * 1e6))\n",
    "\n",
    "print(total_ram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.buffer[0]['next_obs'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EnvironmentInterface():\n",
    "    def __init__(self, action_repeats=1, merge_frames=False):\n",
    "        self.action_repeats = action_repeats\n",
    "        self.merge_frames = merge_frames\n",
    "    \n",
    "    def take_action(self, action, preprocessor=None, epsilon=None):\n",
    "    \n",
    "        if self.merge_frames:\n",
    "            assert self.action_repeats > 1, 'ERROR: EnvironmentInterface: '\\\n",
    "                + 'Cannot merge frames with action_repeats !> 1.'\\\n",
    "                + 'action_repeats = ' + str(self.action_repeats)\n",
    "                \n",
    "        if self.epsilon is not None:\n",
    "            pass\n",
    "    \n",
    "    def reset_env(self, env, preprocessor=None):\n",
    "        \"\"\"Simple wrapper that restarts the environment\"\"\"\n",
    "        obs = env.reset()\n",
    "        episode_time_step = 0\n",
    "        episode_reward = 0\n",
    "        return obs, episode_time_step, episode_reward\n",
    "\n",
    "envInter = EnvironmentInterface(env, action_repeats=4, merge_frames=True)\n",
    "env.take_action(1, preprocessor)\n",
    "\n",
    "###\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "preprocessor.reset_buffer_2d()\n",
    "preprocessor.add_2d(obs)\n",
    "obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run and render a forward pass\n",
    "import time\n",
    "\n",
    "env = gym.make(env_name)\n",
    "obs = env.reset()\n",
    "preprocessor.reset_buffer_2d()\n",
    "preprocessor.add_2d(obs)\n",
    "obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "env.render()\n",
    "reward_sum = 0\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        action = sess.run(DQN1.action, feed_dict={obsPlaceholder : [obs_input]}) \\\n",
    "            if np.random.rand(1) > 0.05 else np.random.randint(a_size)\n",
    "        action = int(action)\n",
    "        assert 'int' in str(type(action))\n",
    "        time.sleep(0.025)\n",
    "        reward = 0\n",
    "        for i in range(4):\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            r = np.clip(r, -1, 1)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "        preprocessor.add_2d(obs)\n",
    "        obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "        env.render()\n",
    "        reward_sum += reward\n",
    "        if done:\n",
    "            print(reward_sum)\n",
    "            reward_sum = 0\n",
    "            obs = env.reset()\n",
    "            preprocessor.reset_buffer_2d()\n",
    "            preprocessor.add_2d(obs)\n",
    "            obs_input = preprocessor.get_buffer_2d()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "\n",
    "# env.render(close=True)\n",
    "print('Terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
