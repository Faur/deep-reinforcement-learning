{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "\n",
    "\n",
    "### ToDo\n",
    "* break training up in smaller batches!\n",
    "\n",
    "# CartPole with Policy Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "Jupyter.keyboard_manager.command_shortcuts.add_shortcut('r', {\n",
    "    help : 'run all cells',\n",
    "    help_index : 'zz',\n",
    "\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_all_cells();\n",
    "        return false;\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Standard libraries\n",
    "\n",
    "# 3rd party libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Custom libraries\n",
    "import CartPole_config as config\n",
    "import utils\n",
    "import networks\n",
    "import Logger\n",
    "\n",
    "# Library setup\n",
    "tf.set_random_seed(int(time.time()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Settings and parameters\n",
    "experiment_name = None\n",
    "# experiment_name = '2017-07-09-(10-16-55)'\n",
    "\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Derrived settings\n",
    "experiment_name = experiment_name or utils.time_str()\n",
    "logdir = './logdir/'+config.env_name+'/PG/' + experiment_name\n",
    "print('logdir: ', logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from tensorflow.contrib.keras.api.keras.layers import Dense, Input\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame = 0\n",
    "episode = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Create model\n",
    "tf.reset_default_graph()\n",
    "max_train_frame = 2e6\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self, render=False):\n",
    "        self.sess = tf.Session()\n",
    "        self.train_interval = 1 # episodes\n",
    "        self.annealer = utils.Annealer(LEARNING_RATE, 0, max_train_frame)\n",
    "        \n",
    "        self.env = gym.make(config.env_name)\n",
    "        self.render = render\n",
    "        self.should_stop = False\n",
    "        \n",
    "        self.obsPH = tf.placeholder(tf.float32, shape=[None]+[config.num_state], name='obsPlaceholder')\n",
    "        self.actionPH = tf.placeholder(tf.int32, shape=[None], name='actionPlaceholder')\n",
    "        self.advantagePH = tf.placeholder(tf.float32, shape=[None], name='advantagePlaceholder')\n",
    "        self.learningRatePH = tf.placeholder(tf.float32, shape=[], name='learningratePlaceholder')\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.graph = self._build_graph(self.learningRatePH)\n",
    "\n",
    "        self.saver = tf.train.Saver(max_to_keep=5)\n",
    "        self.summary_writer = tf.summary.FileWriter(logdir, self.sess.graph)\n",
    "        self.logger = Logger.Logger(logdir)\n",
    "        self.logger.writer = self.summary_writer\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def load_model(self, path):\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.saver.save(self.sess, path)\n",
    "\n",
    "    def _build_model(self):\n",
    "        input_layer = Input(tensor=self.obsPH)\n",
    "        model_layers = networks.build_dense(input_layer, config.layers, name_stem='dense_')\n",
    "        model = Model(inputs=input_layer, outputs=model_layers)\n",
    "        return model\n",
    "\n",
    "    def _build_graph(self, learning_rate):\n",
    "        class PGGraph: pass\n",
    "        graph = PGGraph\n",
    "\n",
    "        action_hot = tf.one_hot(self.actionPH, config.num_action)\n",
    "        with tf.variable_scope('actor'):\n",
    "            logits = Dense(config.num_action, activation='linear')(self.model.output)\n",
    "            graph.action_probs = tf.nn.softmax(logits)\n",
    "            graph.action_prob = tf.reduce_sum(graph.action_probs * action_hot,\n",
    "                                    axis=1, keep_dims=True)\n",
    "\n",
    "        with tf.variable_scope('training'):\n",
    "            graph.loss_policy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=action_hot, logits=logits)\n",
    "            graph.loss_policy = tf.reduce_mean(graph.loss_policy * self.advantagePH)\n",
    "            \n",
    "            graph.loss_entropy = config.loss_entropy_coef * tf.reduce_mean(\n",
    "                graph.action_probs * tf.log(graph.action_probs + config.eps))\n",
    "\n",
    "            graph.loss_total = graph.loss_policy + graph.loss_entropy\n",
    "            \n",
    "            optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=0.99)\n",
    "            grads_and_vars = optimizer.compute_gradients(graph.loss_total)\n",
    "            grads, variables = zip(*grads_and_vars)\n",
    "#             clipped_gradients, _ = zip(*[(tf.clip_by_value(grad, -1., 1.), var)\n",
    "#                              for grad, var in grads_and_vars])\n",
    "            ## WARNING: The output from clip_by_value might be totally wrong!!!\n",
    "            clipped_gradients, _ = (tf.clip_by_global_norm(grads, 1.))\n",
    "    \n",
    "#             grad_check = tf.check_numerics(clipped_gradients, 'check_numerics caught bad numerics')\n",
    "#             try:\n",
    "#                 with tf.control_dependencies([grad_check]):\n",
    "            graph.train_op = optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "#             except InvalidArgument:\n",
    "#                 print('Bad gradients!')\n",
    "        \n",
    "        ## Create summaries\n",
    "        tf.summary.scalar('training/loss_total', graph.loss_total)\n",
    "        tf.summary.scalar('training/loss_policy', graph.loss_policy)\n",
    "        tf.summary.scalar('training/loss_entropy', graph.loss_entropy)\n",
    "\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram('grad_org/'+v.name[:-2], g)\n",
    "                tf.summary.histogram('var/'+v.name[:-2], g)\n",
    "        for g, v in zip(clipped_gradients, variables):\n",
    "            if g is not None:\n",
    "                tf.summary.histogram('grad_clip/'+v.name[:-2], g)\n",
    "\n",
    "        graph.summary = tf.summary.merge_all()\n",
    "        return graph\n",
    "            \n",
    "            \n",
    "    def stop(self):\n",
    "        self.should_stop = True\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\" Takes a single obs, and returns a single action\"\"\"\n",
    "        obs = [obs]\n",
    "        [p] = self.sess.run(\n",
    "                self.graph.action_probs,\n",
    "                feed_dict={self.obsPH : obs})\n",
    "        a = np.random.choice(config.num_action, p=p)\n",
    "        return a\n",
    "    \n",
    "    def run(self, load_model=False):\n",
    "        \n",
    "        try:\n",
    "            if load_model:\n",
    "                self.load_model(logdir)\n",
    "        except:\n",
    "            print(\"Could not find model to load.\")\n",
    "        \n",
    "        done = False\n",
    "        obs = self.env.reset()\n",
    "        experience = [[], [], []]\n",
    "        rewards = []\n",
    "        global frame, episode\n",
    "        try:\n",
    "            while self.should_stop is False:\n",
    "                frame += 1\n",
    "                \n",
    "                action = self.get_action(obs)\n",
    "                obs, reward, done, _ = self.env.step(action)\n",
    "                if self.render: self.env.render()\n",
    "                \n",
    "                # add experience to memory\n",
    "                rewards.append(reward)\n",
    "                experience[0].append(obs)\n",
    "                experience[1].append(action)\n",
    "                \n",
    "                if done:\n",
    "                    self.logger.log_scalar(tag='performance/reward', \n",
    "                                           value=sum(rewards),\n",
    "                                           step=frame)\n",
    "\n",
    "                    episode += 1                    \n",
    "                    if sum(rewards) >= config.env_max_step: # if we win make the advantage positive for all\n",
    "#                         print('sum(rewards) =', sum(rewards))\n",
    "                        dis_r = 0.01 * np.ones_like(rewards)\n",
    "                        dis_r = list(dis_r)\n",
    "                    else: # compute discounted rewards\n",
    "#                         print('Normal')\n",
    "                        dis_r = utils.discount_rewards(rewards, config.gamma)\n",
    "                        dis_r = list(dis_r)\n",
    "                        \n",
    "#                     print('dis_r', type(dis_r), len(dis_r), type(dis_r[9]))\n",
    "#                     print(dis_r)\n",
    "#                     break\n",
    "                    experience[2] += dis_r\n",
    "\n",
    "                    rewards = []                \n",
    "                    done = False\n",
    "                    obs = self.env.reset()\n",
    "                \n",
    "                    if episode % self.train_interval == 0:\n",
    "                        assert len(experience[0]) == len(experience[1]), \\\n",
    "                            \"Error: experience lenghts don't allign\" + str([len(i) for i in experience])\n",
    "                        assert len(experience[0]) == len(experience[2]), \\\n",
    "                            \"Error: experience lenghts don't allign\" + str([len(i) for i in experience])\n",
    "                        self.logger.log_scalar(tag='training/batch_size', \n",
    "                                           value=len(experience[0]),\n",
    "                                           step=frame)\n",
    "\n",
    "                            \n",
    "                        # stack experience\n",
    "                        obs_stack = np.vstack(experience[0])\n",
    "                        action_stack = np.vstack(experience[1])\n",
    "                        action_stack = np.squeeze(action_stack)\n",
    "                        reward_stack = np.vstack(experience[2])\n",
    "                        reward_stack = np.squeeze(reward_stack)\n",
    "                        # normalize discounted rewardrrrr\n",
    "                        \n",
    "                        reward_std = np.std(reward_stack)\n",
    "                        if np.abs(reward_std) > 1e6:\n",
    "                            reward_stack = (reward_stack - np.mean(reward_stack))/reward_std\n",
    "                        else:\n",
    "                            reward_stack = reward_stack - np.mean(reward_stack)\n",
    "#                         plt.plot(reward_stack)\n",
    "#                         break\n",
    "                        experience = [[], [], []]\n",
    "\n",
    "#                         print('obs_stack', obs_stack.shape)\n",
    "#                         print('action_stack', action_stack.shape)\n",
    "#                         print('reward_stack', reward_stack.shape)\n",
    "                        \n",
    "\n",
    "                        _, summary = self.sess.run(\n",
    "                                [self.graph.train_op, self.graph.summary], \n",
    "                                feed_dict={self.obsPH : obs_stack,\n",
    "                                           self.actionPH : action_stack,\n",
    "                                           self.advantagePH : reward_stack,\n",
    "                                           self.learningRatePH : self.annealer.linear(frame)})\n",
    "                        self.logger.log_scalar('training/learning_rate', self.annealer.linear(frame), frame)\n",
    "                        self.summary_writer.add_summary(summary, frame)\n",
    "                        \n",
    "                        if episode % 500 == 0:\n",
    "                            print(frame, 'model saved', logdir)\n",
    "                            self.save_model(logdir + '/model_'+str(frame))\n",
    "                        \n",
    "                        if frame > max_train_frame:\n",
    "                            print('max_train_frame reached')\n",
    "                            self.should_stop = True\n",
    "                \n",
    "#                         if frame > 5e5:\n",
    "#                             self.train_interval = 3\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "        print('Training ended')\n",
    "        agent.env.render(close=True)\n",
    "\n",
    "agent = PolicyGradient(render=False)\n",
    "agent.run(load_model=True)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Slow episode run\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# setup\n",
    "obs = agent.env.reset()\n",
    "done = False\n",
    "reward_sum = 0\n",
    "action_prob = [] # probability of goint left\n",
    "action_chosen = []\n",
    "# values = []\n",
    "# v_max = 0\n",
    "t_max = 50\n",
    "try:\n",
    "    while True:    \n",
    "        [p] = agent.sess.run(\n",
    "                agent.graph.action_probs,\n",
    "                feed_dict={agent.obsPH : [obs]})\n",
    "#         print(p, type(p))\n",
    "        action_prob.append(p[1])\n",
    "#         values.append(v)\n",
    "#         v_max = 1.25*v if v > v_max else v_max\n",
    "        a = np.random.choice(config.num_action, p=p)\n",
    "        action_chosen.append(a)\n",
    "        obs, reward, done, _ = agent.env.step(a)\n",
    "        reward_sum += int(reward)\n",
    "        img = agent.env.render(mode='rgb_array')\n",
    "\n",
    "        \n",
    "        if t_max - reward_sum < 10:\n",
    "            t_max += 50\n",
    "        fig, ax = plt.subplots(2,1)\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].axis('off')\n",
    "        ax[0].set_title(experiment_name + '\\nt = {:5d}, frames = {:7}'.format(reward_sum, frame))\n",
    "\n",
    "        ax[1].set_title('Action, 1 = right')\n",
    "        ax[1].plot([0, t_max],[0.5, 0.5],'k',alpha=0.5)\n",
    "        ax[1].plot(action_prob)\n",
    "        ax[1].plot(action_chosen, 'bo', markeredgewidth=0.0, markersize=4, alpha=0.25)\n",
    "        ax[1].set_xlim([0,t_max])\n",
    "        ax[1].set_ylim([-0.1, 1.1])\n",
    "\n",
    "        if done:\n",
    "#             ax[1,0].plot(ideal_value(np.ones(reward_sum), config.gamma), c='g', alpha=0.75, label='Discounted Reward')\n",
    "#             ax[1,0].legend()\n",
    "#             plt.savefig('tmp/training_graphs' + utils.time_str() + '.png', bbox_inches='tight')\n",
    "            obs = agent.env.reset()\n",
    "            done = False\n",
    "            reward_sum = 0\n",
    "            action_prob = [] # probability of goint left\n",
    "            action_chosen = []\n",
    "#             values = []\n",
    "#             v_max = 0\n",
    "            t_max = 50\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "\n",
    "\n",
    "print('Terminated', reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
