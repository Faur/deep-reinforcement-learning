{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "* Forward pass\n",
    "* Backwards pass\n",
    "* Summaries\n",
    "* Replay buffer\n",
    "* target network\n",
    "* Save model\n",
    "* Breakout \n",
    "* preprocessing\n",
    "* Action repeats\n",
    "* Set max episode length! (double of the mean?)\n",
    "* Encapsulate training in a class (or something!)\n",
    "* **properly use cartpole_config.py**\n",
    "* TensorBoard Summaries\n",
    "    * Track episode lengths!\n",
    "* Learning rate annealing?\n",
    "\n",
    "\n",
    "### Todo\n",
    "* Track variable values!\n",
    "* Let it train + collect data for report!\n",
    "\n",
    "### Notes\n",
    "Inspired by: https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN on CartPole\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "Jupyter.keyboard_manager.command_shortcuts.add_shortcut('r', {\n",
    "    help : 'run all cells',\n",
    "    help_index : 'zz',\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_all_cells();\n",
    "        return false;\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "## Standard libraries\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "## 3rd party\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "## Custom\n",
    "# import modelDQN\n",
    "import networks\n",
    "import utils\n",
    "import CartPole_config as config\n",
    "# import MountainCar_config as config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(str(config.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Script behavior\n",
    "experiment_name = None\n",
    "experiment_name = str(config.layers[0]) + '_'\n",
    "# experiment_name = '2017-07-14-(03-45-47)'\n",
    "IS_DEBUGGING = True # Make progam run faster\n",
    "IS_DEBUGGING = False\n",
    "\n",
    "## Training\n",
    "# learning_rate = 5e-4\n",
    "# learning_rate = 0.00025 # Human paper\n",
    "learning_rate = 0.0001\n",
    "learning_rate = 0.00005\n",
    "# max_train_frame = 2e6\n",
    "max_train_frame = 2e6\n",
    "max_train_frame = 5e6\n",
    "max_episode_frame = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if IS_DEBUGGING:\n",
    "    config.eps_anneal_period = 1e5\n",
    "    config.replay_buffer_size = int(1e4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "## Standard libraries\n",
    "# import os\n",
    "# import sys\n",
    "# sys.path.append(os.path.join('.', '..')) \n",
    "\n",
    "## 3rd party libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib.keras.api.keras.layers import Dense, Input, Conv2D, Flatten\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "## Custom libraries\n",
    "import utils\n",
    "import networks\n",
    "# import CartPole_config as config\n",
    "import Logger\n",
    "\n",
    "class DQN(object):\n",
    "    \"\"\"docstring for DQN\"\"\" \n",
    "    def __init__(self, config, logdir, learning_rate, max_train_frame=5e7, max_episode_frame=int(1e4),\n",
    "                 render=False):\n",
    "        self.should_stop = False\n",
    "        self.frame = -1 # Also create summaries on first run\n",
    "        self.episode = -1\n",
    "        self.replay_buffer_size = config.replay_buffer_size\n",
    "\n",
    "        self.config = config\n",
    "        self.logdir = logdir\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_train_frame = max_train_frame\n",
    "        self.max_episode_frame = max_episode_frame\n",
    "        \n",
    "        self.render = render\n",
    "        \n",
    "        self.obsPH = tf.placeholder(tf.float32, shape=[None]+[self.config.num_state], name='obsPlaceholder')\n",
    "        self.actionPH = tf.placeholder(tf.int32, shape=[None], name='actionPlaceholder')\n",
    "        self.learningRatePH = tf.placeholder(tf.float32, shape=[], name='learningratePlaceholder')\n",
    "        self.targetQPH = tf.placeholder(tf.float32, shape=[None], name='targetQPlaceholder')\n",
    "        self.tauPH = tf.placeholder(tf.float32, shape=[], name='tauPlaceholder')\n",
    "\n",
    "    def build(self):\n",
    "        ## Build helpers\n",
    "        self.env = gym.make(self.config.env_name)\n",
    "        self.lr_annealer = utils.Annealer(self.learning_rate, self.learning_rate/10, self.max_train_frame)\n",
    "        self.eps_annealer = utils.Annealer(self.config.eps_start,\n",
    "                                           self.config.eps_end,\n",
    "                                           self.config.eps_anneal_period)\n",
    "        self.expBuf = utils.Experience_buffer(self.replay_buffer_size)\n",
    "        \n",
    "        ## Build model and main graph\n",
    "        if self.config.model_type == 'dense':\n",
    "            self.DQN_std, self.DQN_tgt = self._build_dense_model()\n",
    "        self.graph = self._build_graph(self.DQN_std, self.DQN_tgt)\n",
    "        \n",
    "        ## Setup and finalize\n",
    "        self.sess = tf.Session()\n",
    "        self.summary_writer = tf.summary.FileWriter(self.logdir, self.sess.graph)\n",
    "        self.logger = Logger.Logger(self.logdir)\n",
    "        self.logger.writer = self.summary_writer\n",
    "        self.saver = tf.train.Saver(max_to_keep=5)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.sess.run(self.graph.update_tgt, {self.tauPH : 1}) # set target graph = std graph\n",
    "        \n",
    "    def _build_dense_model(self):\n",
    "        input_layer = Input(tensor=self.obsPH)\n",
    "        \n",
    "        with tf.variable_scope('DQN_std'): # std graph must be constructed before tgt!\n",
    "            model_layers = networks.build_dense(input_layer, self.config.layers)\n",
    "            DQN_std = Model(inputs=input_layer, outputs=model_layers)\n",
    "        \n",
    "        with tf.variable_scope('DQN_tgt'):\n",
    "            model_layers = networks.build_dense(input_layer, self.config.layers)\n",
    "            DQN_tgt = Model(inputs=input_layer, outputs=model_layers)\n",
    "        \n",
    "        return DQN_std, DQN_tgt\n",
    "    \n",
    "    def _build_graph(self, model_std, model_tgt):\n",
    "        ## TODO: Ideally this should be broken into two part\n",
    "            # _build_graph_forwards  - builds the action and qValue, that is called twice\n",
    "            # _build_graph_backwards - builds the training ops, called once\n",
    "        class Graph: pass\n",
    "        graph = Graph\n",
    "                \n",
    "        with tf.variable_scope('DQN_std'):\n",
    "            ## Create standard graph forward pass\n",
    "            actions_hot = tf.one_hot(self.actionPH, self.config.num_action, \n",
    "                                        dtype=tf.float32, name='actionsOnehot')            \n",
    "            with tf.variable_scope('qValue'):\n",
    "                graph.qValues = Dense(self.config.num_action, activation=None)\\\n",
    "                                    (model_std.output)\n",
    "                graph.qValue = tf.reduce_sum(tf.multiply(graph.qValues, actions_hot), axis=1)\n",
    "            graph.action = tf.argmax(graph.qValues, axis=1, name='argmaxAction')\n",
    "            \n",
    "            ## Create standard graph backwards pass\n",
    "            with tf.variable_scope('training'):\n",
    "                with tf.variable_scope('loss'):\n",
    "                    with tf.variable_scope('q'):\n",
    "                        graph.loss_q = tf.reduce_mean(tf.square(self.targetQPH - graph.qValue))\n",
    "                    graph.loss_total = graph.loss_q\n",
    "                \n",
    "                optimizer = tf.train.RMSPropOptimizer(self.learningRatePH, decay=0.99)\n",
    "#                 grads, variables = zip(* optimizer.compute_gradients(graph.loss_total))\n",
    "#                 clipped_grads, _ = (tf.clip_by_global_norm(grads, 0.1))\n",
    "#                 graph.train_op = optimizer.apply_gradients(zip(clipped_grads, variables))\n",
    "\n",
    "\n",
    "#                 grads_and_vars = optimizer.compute_gradients(graph.loss_total)\n",
    "                grads, variables = zip(* optimizer.compute_gradients(graph.loss_total))\n",
    "                grads = [grad if grad is not None else tf.zeros_like(var) \n",
    "                                      for grad, var in zip(grads, variables)]\n",
    "                    # Make 'none' grads into zeros\n",
    "                clipped_grads_and_vars = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in zip(grads, variables)]\n",
    "                graph.train_op = optimizer.apply_gradients(clipped_grads_and_vars)\n",
    "                \n",
    "        with tf.variable_scope('DQN_tgt'):\n",
    "            ## Create target graph forward pass\n",
    "            with tf.variable_scope('qValue'):\n",
    "                graph.qValues_tgt = Dense(self.config.num_action, activation=None)\\\n",
    "                                    (model_tgt.output)\n",
    "            graph.action_tgt = tf.argmax(graph.qValues_tgt, axis=1, name='argmaxAction')\n",
    "\n",
    "            ## Create target update op\n",
    "            with tf.variable_scope('update'):\n",
    "                std_vars, tgt_vars = [], []\n",
    "                for var in tf.trainable_variables():\n",
    "                    if var.name.startswith('DQN_std'):\n",
    "                        std_vars.append(var)\n",
    "                    elif var.name.startswith('DQN_tgt'):\n",
    "                        tgt_vars.append(var)\n",
    "\n",
    "                graph.update_tgt = []\n",
    "                for std_var, tgt_var in zip(std_vars, tgt_vars):\n",
    "                    op = tgt_var.assign(\n",
    "                        (1-self.tauPH)*tgt_var.value() + self.tauPH*std_var.value())\n",
    "                    graph.update_tgt.append(op)\n",
    "\n",
    "        ## Create summaries\n",
    "        tf.summary.scalar('training/loss_total', graph.loss_total)\n",
    "\n",
    "        for g, v in zip(grads, variables):\n",
    "            if (g is not None) and (v is not None):\n",
    "                tf.summary.histogram('grad_org/'+v.name[:-2], g)\n",
    "#                 tf.summary.histogram('var/'+v.name[:-2], g)\n",
    "        for g, v in clipped_grads_and_vars:\n",
    "            if (g is not None) and (v is not None):\n",
    "                tf.summary.histogram('grad_clip/'+v.name[:-2], g)\n",
    "\n",
    "        graph.summary_op = tf.summary.merge_all()\n",
    "        return graph\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        try:                \n",
    "            ckpt = tf.train.get_checkpoint_state(path)\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        except:\n",
    "            print(\"Could not find model to load.\")\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.saver.save(self.sess, path)\n",
    "\n",
    "    def stop(self):\n",
    "        self.should_stop = True\n",
    "    \n",
    "    def get_action(self, obs):\n",
    "        eps = self.eps_annealer.linear(self.frame)\n",
    "        if np.random.rand(1) > eps:\n",
    "            action = self.sess.run(self.graph.action, feed_dict={self.obsPH : obs})\n",
    "        else:\n",
    "            action = np.random.randint(self.config.num_action)\n",
    "        return int(action)\n",
    "    \n",
    "    def random_fill_experience_buffer(self, min_size=None):\n",
    "        \"\"\" Fill the experience buffer with random actions\"\"\"\n",
    "\n",
    "        obs = self.env.reset()\n",
    "        if min_size is None:\n",
    "            min_size = self.expBuf.buffer_capacity\n",
    "\n",
    "        print('Filling experience buffer:', min(min_size, self.expBuf.buffer_capacity))        \n",
    "        while self.expBuf.buffer_size() <= min_size:\n",
    "            if self.expBuf.is_full():\n",
    "                break\n",
    "            \n",
    "            action = np.random.randint(self.config.num_action)\n",
    "            obs_next, reward, done, _ = self.env.step(action)\n",
    "            reward = np.clip(reward, -1, 1)\n",
    "            done = int(done)\n",
    "            experience = {'obs':[obs], 'action':action, 'reward':reward,\n",
    "                      'next_obs':[obs_next], 'done':done}\n",
    "            self.expBuf.add(experience)\n",
    "                        \n",
    "            if done:\n",
    "                obs = self.env.reset()\n",
    "            \n",
    "            \n",
    "    \n",
    "    def run(self, load_model=False):\n",
    "        if load_model: self.load_model(self.logdir)\n",
    "\n",
    "        try:\n",
    "            self.random_fill_experience_buffer(self.config.replay_min_size)\n",
    "            \n",
    "            print('Begin train loop')\n",
    "            done = False\n",
    "            obs = self.env.reset()\n",
    "            ep_t = 0 # episode time step\n",
    "            ep_r = 0 # episode total reward\n",
    "            ep_r_clip = 0 # episode total reward\n",
    "            while self.should_stop is False:\n",
    "                self.frame += 1\n",
    "                action = self.get_action([obs])\n",
    "#                 print('action', action, type(action))\n",
    "                \n",
    "                obs_next, reward, done, _ = self.env.step(action)\n",
    "                ep_t += 1\n",
    "                ep_r += reward\n",
    "                reward = np.clip(reward, -1, 1)\n",
    "                ep_r_clip += reward\n",
    "                done = int(done)\n",
    "                experience = {'obs':[obs], 'action':action, 'reward':reward,\n",
    "                              'next_obs':[obs_next], 'done':done}\n",
    "                if ep_r != 500: # TODO: Remove this???\n",
    "                    self.expBuf.add(experience)\n",
    "                else:\n",
    "                    # IGNORE TERMINAL DUE TO SUCCESS!\n",
    "                    print('EXP ignored because ep_r == 500!!')\n",
    "                obs = obs_next\n",
    "\n",
    "\n",
    "                if self.frame % self.config.update_freq == 0:\n",
    "                    ## Update parameters\n",
    "                    train_batch = self.expBuf.sample(self.config.batch_size)\n",
    "                    \n",
    "                    [action, qValues_tgt] = self.sess.run([self.graph.action, self.graph.qValues_tgt],\n",
    "                        feed_dict={self.obsPH : train_batch['next_obs']})\n",
    "                    \n",
    "                    qValue_tgt = qValues_tgt[range(self.config.batch_size), action] \n",
    "#                         # The DQN2 Q value of the action chosen by DQN1\n",
    "                    terminal_mask = 1 - train_batch['done'] # Remove qValue_tgt, when at terminal state\n",
    "    \n",
    "                    target = train_batch['reward'] + self.config.gamma*qValue_tgt*terminal_mask\n",
    "\n",
    "                    ## Update DQN1\n",
    "                    DQN1_train_dict = {\n",
    "                                self.obsPH : train_batch['obs'],\n",
    "                                self.actionPH : train_batch['action'],\n",
    "                                self.targetQPH : target,\n",
    "                                self.learningRatePH : self.lr_annealer.linear(self.frame),\n",
    "                                self.tauPH : self.config.tau,\n",
    "                        }\n",
    "                    summary, _, _ = self.sess.run([self.graph.summary_op, self.graph.train_op, self.graph.update_tgt],\n",
    "                                      feed_dict=DQN1_train_dict)\n",
    "\n",
    "                if self.frame % int(1e3) == 0:\n",
    "                    ## Tensorboard logging\n",
    "                    self.summary_writer.add_summary(summary, self.frame)\n",
    "                    self.logger.log_scalar('training/learning_rate', self.lr_annealer.linear(self.frame), self.frame)\n",
    "                    self.logger.log_scalar('training/epsilon', self.eps_annealer.linear(self.frame), self.frame)\n",
    "                    \n",
    "        \n",
    "                if self.frame % int(5e5) == 0\\\n",
    "                        and self.frame > 0:\n",
    "                    ## save model\n",
    "                    self.save_model(self.logdir + '/model_'+str(self.frame))\n",
    "                    print('{:10} model saved:'.format(self.frame), self.logdir)\n",
    "\n",
    "                \n",
    "                if ep_t > self.max_episode_frame:\n",
    "                    done = True\n",
    "                \n",
    "                if done:\n",
    "#                     print(ep_t, ep_r, ep_r_clip)\n",
    "                    self.episode += 1\n",
    "                    self.logger.log_scalar('performance/episode_len', ep_t, self.frame)\n",
    "                    self.logger.log_scalar('performance/reward',      ep_r, self.frame)\n",
    "                    self.logger.log_scalar('performance/reward_clip', ep_r_clip, self.frame)\n",
    "                    self.logger.log_scalar('performance/episodes',    self.episode, self.frame)\n",
    "                    done = False\n",
    "                    obs = self.env.reset()\n",
    "                    ep_t = 0 \n",
    "                    ep_r = 0 \n",
    "                    ep_r_clip = 0 \n",
    "#                     break\n",
    "\n",
    "                if self.frame > self.max_train_frame:\n",
    "                    print('max_train_frame reached')\n",
    "                    self.should_stop = True\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "        print('Training ended')\n",
    "        self.env.render(close=True)\n",
    "    \n",
    "    \n",
    "    def create_video(self, title, target_dir, num_episodes=1, frame_duration=None, figsize=(8,4)):\n",
    "        import imageio\n",
    "\n",
    "        ## Setup\n",
    "        font = FontProperties()\n",
    "        font.set_family('monospace')\n",
    "        episode = 0\n",
    "\n",
    "        ## Initialize\n",
    "        obs = self.env.reset()\n",
    "        done = False\n",
    "        reward_sum = 0\n",
    "        Qs_collection = np.zeros((1, self.config.num_action))\n",
    "        t_max = 50\n",
    "        ep_t = 0\n",
    "        actions = []\n",
    "        try:\n",
    "            with imageio.get_writer(target_dir, duration=frame_duration) as writer:\n",
    "                while episode < num_episodes:\n",
    "                    ep_t += 1\n",
    "                    [Qs] = self.sess.run(\n",
    "                            self.graph.qValues,\n",
    "                            feed_dict={self.obsPH : [obs]})\n",
    "                    action = np.argmax(Qs)\n",
    "                    actions.append(action)\n",
    "                    Qs = np.expand_dims(Qs, axis=0)\n",
    "                    Qs_collection = np.concatenate((Qs_collection, Qs), axis=0)\n",
    "                    \n",
    "                    obs, reward, done, _ = self.env.step(action)\n",
    "                    reward_sum += reward\n",
    "                    img = self.env.render(mode='rgb_array')\n",
    "\n",
    "                    if t_max - ep_t < 10:\n",
    "                        t_max += 50\n",
    "                    \n",
    "                    ## Plotting!\n",
    "                    fig = plt.figure(figsize=figsize, dpi=240)\n",
    "\n",
    "                    ## Image\n",
    "                    ax = fig.add_subplot(221)\n",
    "                    plt.imshow(img)\n",
    "                    ax.yaxis.set_visible(False)\n",
    "                    ax.xaxis.set_ticks_position('none')\n",
    "                    ax.set_xticklabels([])\n",
    "\n",
    "                    ## Text\n",
    "                    ax = fig.add_subplot(222)\n",
    "                    plt.axis('off')\n",
    "                    ax.text(0,0,   'Title:       ' + title\n",
    "                                +'\\nEnvironment: ' + self.config.env_name\n",
    "                                # +'\\nExperiment:  ' + experiment_name # Not available in this scope!\n",
    "                                +'\\nNum. param.  ' + str(utils.num_trainable_param())                        \n",
    "                                +'\\nStep:        ' + str(ep_t)\n",
    "                                +'\\nReward:      ' + str(reward_sum)                            \n",
    "                                +'\\nAction:      ' + str(action)                            \n",
    "                            , fontproperties=font)\n",
    "\n",
    "                    ## Q-value\n",
    "                    fig.add_subplot(223)\n",
    "                    plt.title('Q values')\n",
    "#                     plt.plot([0, t_max],[0.5, 0.5],'k',alpha=0.5)\n",
    "                    for i in range(Qs.shape[1]):\n",
    "                        plt.plot(Qs_collection[1:,i],)\n",
    "#                     plt.plot(action_chosen, 'bo', markeredgewidth=0.0, markersize=4, alpha=0.25)\n",
    "                    plt.xlim([0,t_max])\n",
    "                    \n",
    "                    ## Action\n",
    "                    fig.add_subplot(224)\n",
    "                    plt.title('Action')\n",
    "                    plt.plot(actions)\n",
    "                    plt.ylim([-0.1, 1.1])\n",
    "                    plt.xlim([0,t_max])\n",
    "\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    fig.canvas.draw()\n",
    "                    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "                    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "                    writer.append_data(data)\n",
    "#                     plt.close(fig)\n",
    "                    from IPython.display import clear_output\n",
    "                    clear_output(wait=True)\n",
    "                    plt.show()\n",
    "\n",
    "                    if done:\n",
    "                        episode += 1\n",
    "                        Qs_collection = np.zeros((1,2))\n",
    "                        print('Episode {:3}, frames {:4}'.format(episode, reward_sum))\n",
    "                        obs = self.env.reset()\n",
    "                        done = False\n",
    "                        reward_sum = 0\n",
    "                        t_max = 50\n",
    "                        ep_t = 0\n",
    "                        actions = []\n",
    "        except KeyboardInterrupt:\n",
    "            print('KeyboardInterrupt')\n",
    "        self.env.render(close=True)\n",
    "\n",
    "\n",
    "# try:\n",
    "#     agent.run(load_model=True)\n",
    "# except NameError:\n",
    "#     print('Creating new model')\n",
    "#     agent = DQN(config, logdir, learning_rate, max_train_frame=max_train_frame, max_episode_frame=max_episode_frame)\n",
    "#     agent.build()\n",
    "#     agent.run(load_model=True)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Derived settings\n",
    "for i in range(3):\n",
    "    print('i', i)\n",
    "#     run_name = experiment_name or utils.time_str()\n",
    "    run_name = experiment_name + utils.time_str()\n",
    "    logdir = './logdir/'+config.env_name+'/DQN/' + run_name\n",
    "    print('logdir\\t\\t', logdir)\n",
    "    if IS_DEBUGGING: logdir += '_debug'\n",
    "\n",
    "    ## Setup\n",
    "    tf.reset_default_graph()\n",
    "    random_seed = int(time.time())\n",
    "    tf.set_random_seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    print('random seed\\t', random_seed)\n",
    "\n",
    "    print('Creating new model')\n",
    "    agent = DQN(config, logdir, learning_rate, max_train_frame=max_train_frame, max_episode_frame=max_episode_frame)\n",
    "    agent.build()\n",
    "    agent.run(load_model=False)\n",
    "\n",
    "    agent.create_video('DQN', './tmp/CP_DQN_' + run_name + '.gif')\n",
    "    \n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent.create_video('DQN', './tmp/DQN.gif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# agent.run(load_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
