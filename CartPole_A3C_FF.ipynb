{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done\n",
    "* TensorBoard Summaries\n",
    "\n",
    "\n",
    "### Missing\n",
    "* Learning rate annealing\n",
    "* FFNN\n",
    "* RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "Jupyter.keyboard_manager.command_shortcuts.add_shortcut('r', {\n",
    "    help : 'run all cells',\n",
    "    help_index : 'zz',\n",
    "\n",
    "    handler : function (event) {\n",
    "        IPython.notebook.execute_all_cells();\n",
    "        return false;\n",
    "    }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# standard libraries\n",
    "import os\n",
    "\n",
    "# 3rd party libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# from tensorflow.contrib.keras.python.keras import backend as K\n",
    "\n",
    "# Custom libraries\n",
    "import CartPole_config as config\n",
    "import utils\n",
    "# import networks\n",
    "# # import A3C\n",
    "from Logger import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "experiment_name = None\n",
    "# experiment_name = 'LR_annealing_less_shit'\n",
    "\n",
    "load_model = False\n",
    "# wipe_records = not load_model\n",
    "\n",
    "## Hyperparameters\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# Script behavior\n",
    "Tmax = 100\n",
    "tmax = 5\n",
    "T = 0\n",
    "\n",
    "# Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Derived parameters\n",
    "experiment_name = experiment_name or utils.time_str()\n",
    "logdir = './logdir/'+config.env_name+'/A3C/' + experiment_name\n",
    "print('logdir: ', logdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from tensorflow.contrib.keras.api.keras.layers import Dense, Input\n",
    "from tensorflow.contrib.keras.api.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "class ActorCritic:\n",
    "    \"\"\" ActorCritic class encapsulates the neural network, TensorFlow \n",
    "        graph definition and related computation for an actor-critic model\n",
    "    \"\"\"\n",
    "    __name__ = 'ActorCritic'\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        from tensorflow.contrib.keras.python.keras.backend import set_session, manual_variable_initialization\n",
    "        set_session(self.sess)\n",
    "        manual_variable_initialization(True)\n",
    "        \n",
    "        self.obsPH = tf.placeholder(tf.float32, shape=[None]+[config.num_state], name='obsPlaceholder')\n",
    "        self.actionPH = tf.placeholder(tf.int32, shape=[None, 1], name='actionPlaceholder')\n",
    "        self.discoutedRewardPH = tf.placeholder(tf.float32, shape=[None, 1], name='discoutedRewardPlaceholder')\n",
    "        self.learningRatePH = tf.placeholder(tf.float32, shape=[], name='learningRatePlaceholder')\n",
    "        \n",
    "        self.model = self._build_model()\n",
    "        self.graph = self._build_graph()\n",
    "        self.saver = tf.train.Saver(max_to_keep=5)\n",
    "        self.summary_writer = tf.summary.FileWriter(logdir, self.sess.graph)\n",
    "        self.logger = Logger(logdir)\n",
    "        self.logger.writer = self.summary_writer\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.default_graph = tf.get_default_graph()\n",
    "        self.default_graph.finalize() # no more modification!\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.saver.save(self.sess, path)\n",
    "    \n",
    "    def _build_model(self):\n",
    "        input_layer = Input(tensor=self.obsPH)\n",
    "        model_layers = networks.build_dense(input_layer, config.layers, name_stem='stem_')\n",
    "        model = Model(inputs=input_layer, outputs=model_layers)\n",
    "        return model\n",
    "\n",
    "    def _build_graph(self):\n",
    "        class ACGraph: pass; \n",
    "        graph = ACGraph\n",
    "\n",
    "        with tf.variable_scope('critic'):\n",
    "            graph.value = Dense(1, activation='linear')(self.model.output)\n",
    "            graph.advantage = self.discoutedRewardPH - graph.value\n",
    "            graph.loss_value = config.loss_v_coef * tf.reduce_mean(tf.square(graph.advantage))\n",
    "#             print('graph.value\\t', graph.value.get_shape())\n",
    "#             print('graph.advantage\\t', graph.advantage.get_shape())\n",
    "#             print('graph.loss_value\\t', graph.loss_value.get_shape())\n",
    "#             print()\n",
    "                \n",
    "        graph.action_hot = tf.one_hot(self.actionPH, depth=config.num_action)\n",
    "        graph.action_hot = tf.squeeze(graph.action_hot, axis=1)\n",
    "            # TODO: this is shit!\n",
    "        with tf.variable_scope('actor'):\n",
    "            with tf.variable_scope('action'):\n",
    "                graph.action_probs = Dense(config.num_action, \n",
    "                                       activation='softmax')(self.model.output)\n",
    "    #             graph.action_prob = tf.reduce_sum(graph.action_probs * action_hot, \n",
    "    #                                     axis=1, keep_dims=True)\n",
    "                graph.action_pre_sum = tf.multiply(graph.action_probs, graph.action_hot)\n",
    "                graph.action_prob = tf.reduce_sum(graph.action_pre_sum, \n",
    "                                                  axis=1,keep_dims=True)\n",
    "            with tf.variable_scope('loss'):\n",
    "                log_prob = tf.log(graph.action_prob + config.eps) # log probability of selected action (scalar)\n",
    "                graph.loss_policy = - tf.reduce_mean(log_prob * tf.stop_gradient(graph.advantage))\n",
    "                graph.loss_entropy = config.loss_entropy_coef * tf.reduce_mean(\n",
    "                    graph.action_probs * tf.log(graph.action_probs + config.eps))\n",
    "#             print('action_hot\\t', graph.action_hot.get_shape())            \n",
    "#             print('graph.action_probs\\t', graph.action_probs.get_shape())            \n",
    "#             print('graph.action_prob\\t', graph.action_prob.get_shape())\n",
    "#             print('log_prob\\t', log_prob.get_shape())\n",
    "#             print()\n",
    "\n",
    "#             print('graph.loss_policy\\t', graph.loss_policy.get_shape())\n",
    "#             print('graph.loss_entropy\\t', graph.loss_entropy.get_shape())\n",
    "\n",
    "        graph.loss_total = graph.loss_policy + graph.loss_value + graph.loss_entropy\n",
    "#         print('graph.loss_total\\t', graph.loss_total.get_shape())\n",
    "#         print()\n",
    "\n",
    "        optimizer = tf.train.RMSPropOptimizer(self.learningRatePH, decay=0.99)\n",
    "        grads_and_vars = optimizer.compute_gradients(graph.loss_total)\n",
    "        grads, variables = zip(*grads_and_vars)\n",
    "#         clipped_gradients = [(tf.clip_by_value(grad, -1., 1.), var)\n",
    "#                          for grad, var in grads_and_vars]\n",
    "        clipped_gradients, _ = (tf.clip_by_global_norm(grads, 1.))\n",
    "        graph.train_op = optimizer.apply_gradients(zip(clipped_gradients, variables))\n",
    "\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram('grad_org/'+v.name[:-2], g)\n",
    "                tf.summary.histogram('var/'+v.name[:-2], g)\n",
    "        for g, v in zip(clipped_gradients, variables):\n",
    "            if g is not None:\n",
    "                tf.summary.histogram('grad_clip/'+v.name[:-2], g)\n",
    "\n",
    "\n",
    "#         optimizer = tf.train.RMSPropOptimizer(learning_rate, decay=0.99)\n",
    "#         graph.train_op = optimizer.minimize(graph.loss_total)\n",
    "\n",
    "        tf.summary.scalar('training/loss_value', graph.loss_value)\n",
    "        tf.summary.scalar('training/loss_policy', graph.loss_policy)\n",
    "        tf.summary.scalar('training/loss_entropy', graph.loss_entropy)\n",
    "        tf.summary.scalar('training/loss_total', graph.loss_total)\n",
    "\n",
    "#         tf.summary.scalar('performance/value', graph.value)\n",
    "#         tf.summary.scalar('performance/discoutedReward', self.discoutedRewardPH)\n",
    "#         tf.summary.scalar('performance/action_prob', graph.action_prob)\n",
    "        graph.summary = tf.summary.merge_all()\n",
    "        return graph\n",
    "\n",
    "class TrainingMemory:\n",
    "    __name__ = 'TrainingMemory'\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, keep=None):\n",
    "        if keep is None:\n",
    "            self.obs = []\n",
    "            self.action = []\n",
    "            self.reward = []\n",
    "            self.obs_next = []\n",
    "#             self.obs_mask = []\n",
    "            self.eps_t = []\n",
    "        else: # keep the last 'keep' experiences\n",
    "            num_to_delete = self.size - keep\n",
    "            self.obs = self.obs[num_to_delete:]\n",
    "            self.action = self.action[num_to_delete:]\n",
    "            self.reward = self.reward[num_to_delete:]\n",
    "            self.obs_next = self.obs_next[num_to_delete:]            \n",
    "#             self.obs_mask = self.obs_mask[num_to_delete:]\n",
    "            self.eps_t = self.eps_t[num_to_delete:]\n",
    "    \n",
    "    def add(self, obs, action, reward, obs_next, done, eps_t):\n",
    "        self.obs.append(obs)\n",
    "        self.action.append(action)\n",
    "        self.reward.append(reward)\n",
    "        self.eps_t.append(eps_t)\n",
    "        if done:\n",
    "            self.obs_next.append(obs_next)\n",
    "#             self.obs_next.append(np.zeros_like(obs)) # just a dummy!\n",
    "#             self.obs_mask.append(0.)\n",
    "        else:\n",
    "            self.obs_next.append(obs_next)\n",
    "#             self.obs_mask.append(1.)\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return len(self.obs)\n",
    "    \n",
    "    def test(self):\n",
    "        pass\n",
    "#         print('Testing', self.__name__)\n",
    "#         memory = TrainingMemory()\n",
    "#         utils.print_attributes(memory)\n",
    "\n",
    "#         memory.add(obs=1, action=2, reward=3, obs_next=4, done=False)\n",
    "#         memory.add(obs=1, action=2, reward=3, obs_next=4, done=False)\n",
    "#         memory.add(obs=1, action=2, reward=3, obs_next=4, done=False)\n",
    "#         memory.add(obs=1, action=2, reward=3, obs_next=4, done=False)\n",
    "#         memory.add(obs=1, action=2, reward=3, obs_next=4, done=True)\n",
    "#         print('size', memory.size)\n",
    "#         print(memory.obs)\n",
    "#         print(memory.action)\n",
    "#         print(memory.reward)\n",
    "#         print(memory.obs_next)\n",
    "#         print(memory.obs_mask)\n",
    "\n",
    "#         memory.reset(keep=2)\n",
    "#         print('size', memory.size)\n",
    "#         print(memory.obs)\n",
    "#         print(memory.action)\n",
    "#         print(memory.reward)\n",
    "#         print(memory.obs_next)\n",
    "#         print(memory.obs_mask)\n",
    "\n",
    "# TrainingMemory.test(TrainingMemory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "n_max = 3 # largest n in n-step rewards\n",
    "NUM_FRAMES = 0 # should probably be added to the brain\n",
    "EPS_NUM = 0\n",
    "max_train_frames = int(1e7) # used for learning rate annealing\n",
    "# max_train_frames = np.inf\n",
    "\n",
    "# EPS_START = 1\n",
    "# EPS_STOP = 0.0\n",
    "# EPS_STEPS = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "\n",
    "class A3C_agent(threading.Thread):\n",
    "    \"\"\" Holds a copy of the environment and trains on it asynchroneously\"\"\"\n",
    "    stop_signal = False\n",
    "    \n",
    "    \n",
    "    def __init__(self, brain, number, render=False, train=True):\n",
    "        threading.Thread.__init__(self)\n",
    "        self.env = gym.make(config.env_name)\n",
    "        self.memory = TrainingMemory()\n",
    "        \n",
    "        self.number = number\n",
    "        self.name = 'A3Cworker_' + str(self.number)\n",
    "        self.brain = brain\n",
    "        self.render = render\n",
    "        self.train = train\n",
    "        \n",
    "        \n",
    "    def get_action(self, obs):\n",
    "        \"\"\" Takes a single obs, and returns a single action\"\"\"\n",
    "        obs = [obs]\n",
    "        [p] = self.brain.sess.run(\n",
    "                self.brain.graph.action_probs,\n",
    "                feed_dict={self.brain.obsPH : obs})\n",
    "        a = np.random.choice(config.num_action, p=p)\n",
    "        return a\n",
    "\n",
    "    def discount_r_terminal(self, r):\n",
    "        \"\"\" Compute the true total discounted reward, no estimation\"\"\"\n",
    "        r_dis = np.zeros_like(r)\n",
    "        \n",
    "        for i in reversed(range(len(r))):\n",
    "            if i == len(r)-1:\n",
    "                r_dis[i] = r[i]\n",
    "            else:\n",
    "                r_dis[i] = r[i] + config.gamma*r_dis[i+1]\n",
    "\n",
    "#         print('discount_r_terminal')\n",
    "#         print('r_dis', r_dis.shape)\n",
    "#         print(r_dis)\n",
    "#         print()\n",
    "        \n",
    "        return r_dis\n",
    "\n",
    "    def discount_r_n_step(self, r, s_next, n_max):\n",
    "        \"\"\" r_dis(t) = \"\"\"\n",
    "        \n",
    "        r_dis = np.zeros_like(r[:batch_size])\n",
    "        v = self.brain.sess.run(\n",
    "            self.brain.graph.value,\n",
    "            feed_dict={self.brain.obsPH : s_next})\n",
    "\n",
    "        for i in range(len(r_dis)):\n",
    "            for n in range(n_max):\n",
    "                r_dis[i] += config.gamma**n * r[i]\n",
    "            r_dis[i] += config.gamma**n_max * v[i + n_max - 1]\n",
    "                # v is computed from s_next which is already t+1\n",
    "\n",
    "#         print('discount_r_n_step')\n",
    "#         print('r_dis', r_dis.shape)\n",
    "#         print(r_dis)\n",
    "#         print('v', v.shape)\n",
    "#         print(v)\n",
    "#         print()\n",
    "\n",
    "        return r_dis\n",
    "    \n",
    "    def get_train_batch(self, terminal, n_max):\n",
    "        assert self.memory.size > 0, 'Error: self.memory.size = ' + str(self.memory.size)\n",
    "        s = np.vstack(self.memory.obs)\n",
    "        a = np.vstack(self.memory.action)\n",
    "        r = np.vstack(self.memory.reward)\n",
    "\n",
    "        if terminal:\n",
    "            r_dis = self.discount_r_terminal(r)\n",
    "            assert len(r_dis) == self.memory.size, 'Error: len(r_dis)=' + str(len_r_dis) + ', self.memory.size=' + str(self.memory.size)\n",
    "            return s, a, r_dis\n",
    "        else:\n",
    "            # Full n-step implementation\n",
    "            s_next = np.vstack(self.memory.obs_next)\n",
    "            r_dis = self.discount_r_n_step(r, s_next, n_max)\n",
    "\n",
    "            # TODO: implement the multi-step returns\n",
    "            assert len(r_dis) == batch_size, \\\n",
    "                \"Error, len(r_dis) should be batch_size, \"+str(batch_size) \\\n",
    "                + \", but it is\"+len(r_dis)\n",
    "            return s[:batch_size], a[:batch_size], r_dis\n",
    "    \n",
    "        # return length should \n",
    "\n",
    "    \n",
    "    def update_weights(self, n_max, terminal):\n",
    "        \"\"\" self.memory may only contain consecutive experience from within\n",
    "            the same episode\n",
    "        \"\"\"\n",
    "        s, a, r_dis = self.get_train_batch(terminal, n_max)                    \n",
    "\n",
    "        \n",
    "#         print('update_weights, terminal = ', terminal)\n",
    "#         print('s', s.shape)\n",
    "#         print('a', a.shape)\n",
    "#         print('r_dis', r_dis.shape)\n",
    "#         print(r_dis)\n",
    "#         v = self.brain.sess.run(\n",
    "#             self.brain.graph.value,\n",
    "#             feed_dict={self.brain.obsPH : s})\n",
    "#         print('v', v.shape)\n",
    "#         print(v)\n",
    "#         print()\n",
    "\n",
    "\n",
    "        _, summary = self.brain.sess.run(\n",
    "            [self.brain.graph.train_op, self.brain.graph.summary], \n",
    "            feed_dict={self.brain.obsPH : s, \n",
    "                       self.brain.actionPH : a,\n",
    "                       self.brain.discoutedRewardPH : r_dis,\n",
    "                       self.brain.learningRatePH : LEARNING_RATE})\n",
    "        return summary\n",
    "            \n",
    "    def stop(self):\n",
    "        self.stop_signal = True\n",
    "\n",
    "    def run(self):\n",
    "        obs = self.env.reset()\n",
    "        eps_frames = 0\n",
    "        eps_reward = 0\n",
    "        eps_reward_sum = []\n",
    "        eps_num = 0\n",
    "        global NUM_FRAMES, EPS_NUM, LEARNING_RATE\n",
    "        while self.stop_signal == False:\n",
    "            try:\n",
    "                eps_frames += 1\n",
    "                ## Get experience\n",
    "                action = self.get_action(obs)\n",
    "                obs_new, reward, done, _ = self.env.step(action)\n",
    "                if self.render: self.env.render()\n",
    "\n",
    "                ## Save experience\n",
    "                reward = np.clip(reward, -1, 1)\n",
    "                eps_reward += reward\n",
    "                self.memory.add(obs=obs, action=action, reward=reward, \n",
    "                                obs_next=obs_new, done=done, eps_t=eps_frames)\n",
    "                obs = obs_new\n",
    "\n",
    "                # [TRAIN]\n",
    "                if done:\n",
    "                    NUM_FRAMES += self.memory.size\n",
    "                    num_frames = NUM_FRAMES\n",
    "#                     print('done, {:4}, {:4}, {}'.format(EPS_NUM, eps_frames, num_frames))\n",
    "                    summary = self.update_weights(n_max, terminal=done)\n",
    "                    self.memory.reset()\n",
    "                    self.brain.logger.log_scalar(tag='performance/eps_reward', value=eps_reward, step=num_frames)\n",
    "                    \n",
    "                    obs = self.env.reset()\n",
    "                    eps_frames = 0\n",
    "                    eps_reward_sum += [eps_reward]\n",
    "                    eps_reward = 0\n",
    "                    save_interval = 200\n",
    "                    EPS_NUM += 1\n",
    "                    if EPS_NUM % save_interval == 0 and EPS_NUM > 0:\n",
    "                        print(self.name+': {:5}, {:7.2f}, {:8}, model saved'.format(\n",
    "                                EPS_NUM, np.mean(eps_reward_sum), num_frames))\n",
    "                        self.brain.save_model(logdir + '/model_'+str(EPS_NUM))\n",
    "                        self.brain.logger.log_histogram(tag='train/eps_rewards', \n",
    "                                     values=np.array(eps_reward_sum),\n",
    "                                     step=num_frames)\n",
    "                        eps_reward_sum = []\n",
    "                        LEARNING_RATE = LEARNING_RATE - LEARNING_RATE * num_frames/max_train_frames\n",
    "                        self.brain.logger.log_scalar(tag='training/learning_rate', value=LEARNING_RATE, step=num_frames)\n",
    "\n",
    "#                         break\n",
    "                    self.brain.summary_writer.add_summary(summary, num_frames)\n",
    "#                     break\n",
    "\n",
    "\n",
    "                if self.memory.size >= batch_size + n_max: # We only train where we can get full n_max step returns\n",
    "                    NUM_FRAMES += batch_size\n",
    "                    num_frames = NUM_FRAMES\n",
    "#                     print('memory full', eps_frames, num_frames)\n",
    "                    summary = self.update_weights(n_max, terminal=done)\n",
    "                    self.memory.reset(keep=n_max)\n",
    "                    self.brain.summary_writer.add_summary(summary, num_frames)\n",
    "#                     break\n",
    "\n",
    "                if NUM_FRAMES >= max_train_frames:\n",
    "                    print('NUM_FRAMES >= max_train_frames')\n",
    "                    self.stop_signal = True\n",
    "            except KeyboardInterrupt:\n",
    "                print('KeyboardInterrupt')\n",
    "                self.stop_signal = True\n",
    "        print('Training stopped')\n",
    "\n",
    "print('complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Create a shared brain\n",
    "print('logdir', logdir)\n",
    "tf.reset_default_graph()\n",
    "shared_brain = ActorCritic()\n",
    "try:\n",
    "    shared_brain.load_model(logdir)\n",
    "except AttributeError:\n",
    "    print(\"Couldn't find a model to load\")\n",
    "\n",
    "agent = A3C_agent(shared_brain, number=0, render=False)\n",
    "agent.stop_signal=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Start agents in the background\n",
    "import multi_processing\n",
    "num_agents = 3\n",
    "num_workers = -1 + multiprocessing.cpu_count() # Set workers ot number of available CPU threads\n",
    "\n",
    "\n",
    "agents = [A3C_agent(shared_brain, number=i+1, render=False) for i in range(num_agents)]\n",
    "for a in agents:\n",
    "    a.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Start agent in the foreground\n",
    "agent.run()\n",
    "agent.env.render(close=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # import time\n",
    "# # time.sleep(10)\n",
    "# agent.env.render(close=True)\n",
    "\n",
    "# for a in agents:\n",
    "#     a.stop()\n",
    "# for a in agents:\n",
    "#     a.join()\n",
    "\n",
    "# print('Terminated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Slow episode run\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "# setup\n",
    "obs = agent.env.reset()\n",
    "done = False\n",
    "reward_sum = 0\n",
    "action_prob = [] # probability of goint left\n",
    "action_chosen = []\n",
    "values = []\n",
    "v_max = 0\n",
    "t_max = 50\n",
    "try:\n",
    "    while True:    \n",
    "        \n",
    "        [p], [v] = agent.brain.sess.run(\n",
    "                [agent.brain.graph.action_probs, agent.brain.graph.value],\n",
    "                feed_dict={agent.brain.obsPH : [obs]})\n",
    "        action_prob.append(p[1])\n",
    "        values.append(v)\n",
    "        v_max = 1.25*v if v > v_max else v_max\n",
    "        a = np.random.choice(config.num_action, p=p)\n",
    "        action_chosen.append(a)\n",
    "        obs, reward, done, _ = agent.env.step(a)\n",
    "        reward_sum += int(reward)\n",
    "        img = agent.env.render(mode='rgb_array')\n",
    "\n",
    "        \n",
    "        if t_max - reward_sum < 10:\n",
    "            t_max += 50\n",
    "        fig, ax = plt.subplots(2,2)\n",
    "        ax[0][0].imshow(img)\n",
    "        ax[0][0].axis('off')\n",
    "        ax[0,0].set_title(experiment_name + '\\nt = {:5d}, frames = {:7}'.format(reward_sum, NUM_FRAMES))\n",
    "\n",
    "        ax[0,1].plot([0, t_max],[0.5, 0.5],'k',alpha=0.5)\n",
    "        ax[0,1].plot(action_prob)\n",
    "        ax[0,1].plot(action_chosen, 'bo', markeredgewidth=0.0, markersize=3,alpha=0.75)\n",
    "\n",
    "        ax[0,1].set_xlim([0,t_max])\n",
    "        ax[0,1].set_ylim([-0.1, 1.1])\n",
    "        ax[0,1].set_title('Action, 1 = right')\n",
    "\n",
    "        \n",
    "        ax[1,0].plot(values, c='b', label='Value function')\n",
    "        ax[1,0].set_ylim([0, 1.25 * v_max])\n",
    "        ax[1,0].set_xlim([0,t_max])\n",
    "        ax[1,0].set_title(\"Value function\")\n",
    "\n",
    "        ax[1,1].axis('off')\n",
    "        ax[1,1].set_title('LR: {}'.format(LEARNING_RATE))\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if done:\n",
    "            ax[1,0].plot(ideal_value(np.ones(reward_sum), config.gamma), c='g', alpha=0.75, label='Discounted Reward')\n",
    "#             ax[1,0].legend()\n",
    "            plt.savefig('tmp/training_graphs' + utils.time_str() + '.png', bbox_inches='tight')\n",
    "            obs = agent.env.reset()\n",
    "            done = False\n",
    "            reward_sum = 0\n",
    "            action_prob = [] # probability of goint left\n",
    "            action_chosen = []\n",
    "            values = []\n",
    "            v_max = 0\n",
    "            t_max = 50\n",
    "            \n",
    "        clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('KeyboardInterrupt')\n",
    "\n",
    "\n",
    "print('Terminated', reward_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Discounted rewards experiments\n",
    "\n",
    "# rr = [1,1,1,1,1,1,1,1,1,1]\n",
    "# d1 = []\n",
    "# d2 = []\n",
    "# R1 = []\n",
    "# R2 = []\n",
    "# for i in range(len(rr)):\n",
    "# #     print(i)\n",
    "#     if i == 0: \n",
    "#         d1.append(rr[i])\n",
    "#         d2.append(rr[i])\n",
    "#     elif i == 1:\n",
    "#         d1.append(rr[i] + 0.99*d1[i-1]) # standard\n",
    "#         d2.append(rr[i] + 0.99*d2[i-1])\n",
    "#     else:\n",
    "#         d1.append(rr[i] + 0.99*d1[i-1]) # standard\n",
    "#         d2.append(rr[i] + 0.99*d2[i-1])\n",
    "    \n",
    "    \n",
    "# rr = reversed(rr)\n",
    "# d1 = reversed(d1)\n",
    "\n",
    "\n",
    "# def print_list(string, vals):\n",
    "#     print(string, end=': ')\n",
    "#     for v in vals:\n",
    "#         print('{:5.2f}'.format(v), end=', ')\n",
    "#     print()\n",
    "\n",
    "# print_list('rr', rr)\n",
    "# print_list('d1', d1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
